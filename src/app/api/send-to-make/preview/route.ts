import { NextResponse } from 'next/server';
import OpenAI from 'openai';

// Initialiser OpenAI avec ta cl√© API
const openai = new OpenAI({
  apiKey: process.env.OPENAI_API_KEY
});

// Types pour les options de configuration
interface PreviewRequestBody {
  title: string;
  content: string;
  model?: string;
  maxTokens?: number;
}

// Liste des mod√®les disponibles
const AVAILABLE_MODELS: Record<string, string> = {
  "gpt-3.5-turbo": "GPT-3.5 Turbo",
  "gpt-4": "GPT-4",
  "gpt-4-turbo": "GPT-4 Turbo"
};

// Mod√®les qui supportent le formatage JSON
const SUPPORTS_JSON_FORMAT = ["gpt-3.5-turbo"];

// Mod√®le et tokens par d√©faut
const DEFAULT_MODEL = "gpt-3.5-turbo";
const DEFAULT_MAX_TOKENS = 500;

export async function POST(request: Request) {
  try {
    const body: PreviewRequestBody = await request.json();
    console.log('üìù Contenu re√ßu:', body);
    
    if (!process.env.OPENAI_API_KEY) {
      throw new Error('Cl√© API OpenAI non configur√©e');
    }

    // Utiliser le mod√®le sp√©cifi√© ou le mod√®le par d√©faut
    const model = body.model && AVAILABLE_MODELS[body.model] ? body.model : DEFAULT_MODEL;
    
    // Utiliser maxTokens sp√©cifi√© ou la valeur par d√©faut
    const maxTokens = body.maxTokens || DEFAULT_MAX_TOKENS;

    console.log(`ü§ñ Utilisation du mod√®le: ${model}, max tokens: ${maxTokens}`);

    // Configuration de base
    const requestConfig: any = {
      model: model,
      max_tokens: maxTokens,
      messages: [
        {
          role: "system",
          content: "Tu es un expert en r√©daction LinkedIn. Tu dois optimiser les posts pour maximiser l'engagement tout en gardant un ton professionnel. Tu DOIS r√©pondre uniquement avec un objet JSON contenant 'title' et 'content'."
        },
        {
          role: "user",
          content: `R√©dige un post LinkedIn professionnel en fran√ßais bas√© sur les informations suivantes :
- Titre : ${body.title}
- Contenu : ${body.content}
Assure-toi que le ton est professionnel, engageant et adapt√© √† une audience LinkedIn.${SUPPORTS_JSON_FORMAT.includes(model) ? '' : '\nFormate ta r√©ponse sous forme d\'objet JSON avec les propri√©t√©s "title" et "content".'}`
        }
      ]
    };

    // Ajouter le format de r√©ponse JSON uniquement pour les mod√®les qui le supportent
    if (SUPPORTS_JSON_FORMAT.includes(model)) {
      requestConfig.response_format = { type: "json_object" };
    }

    // Appeler OpenAI directement
    const completion = await openai.chat.completions.create(requestConfig);

    // Extraire la r√©ponse
    const content = completion.choices[0].message.content;
    if (!content) {
      throw new Error('R√©ponse OpenAI vide');
    }

    let result;
    try {
      // Tenter de parser le JSON
      result = JSON.parse(content);
    } catch (parseError) {
      console.error('‚ö†Ô∏è Erreur de parsing JSON:', parseError);
      // Si le parsing √©choue, extraire manuellement title et content avec des regex
      const titleMatch = content.match(/["']title["']\s*:\s*["'](.+?)["']/);
      const contentMatch = content.match(/["']content["']\s*:\s*["'](.+?)["']/);
      
      if (titleMatch && contentMatch) {
        result = {
          title: titleMatch[1],
          content: contentMatch[1]
        };
      } else {
        // Fallback: renvoyer le contenu brut
        result = {
          title: body.title,
          content: content
        };
      }
    }
    
    console.log('‚úÖ Contenu optimis√©:', result);

    return NextResponse.json({
      ...result,
      _meta: {
        model: model,
        maxTokens: maxTokens,
        usage: completion.usage
      }
    });
  } catch (error) {
    console.error('üí• Erreur:', error);
    return NextResponse.json(
      { error: error instanceof Error ? error.message : 'Erreur inconnue' },
      { status: 500 }
    );
  }
} 